# -*- coding: utf-8 -*-
"""Applied_data_science2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BPjgFf4Qni0sUR5NZmBadhD2n9BgYoBD
"""

import pandas as pd
from sklearn import preprocessing
from sklearn.datasets import load_digits
from sklearn.feature_selection import SelectKBest, chi2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.pyplot import figure
from scipy.stats import skewnorm
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_classif
from matplotlib import pyplot
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SequentialFeatureSelector, RFE
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
import sklearn.metrics as metrics
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.tree import DecisionTreeClassifier

"""# **Data** **Normalization**"""

Data = pd.read_csv ('/content/heart.csv')
Data=Data.dropna()
x = Data.iloc[:,0:13].values
y = Data.iloc[:,13].values  

x=pd.DataFrame(x)
y=pd.DataFrame(y)

# adding column name to the respective columns
x.columns =Data.columns[:-1]

x['age']

"""**BOX** **PLOT**"""

def Box_plot(feature):
    figure(figsize=(8,2), dpi=80)
    #Creating plot
    plt.boxplot(x[feature])
    
    # show plot
    plt.show()
    print(feature)

for i in x.columns:
  Box_plot(i)



"""**Features** **Skew**"""

def Skew(feature):
    fig, ax = plt.subplots(1, 3, figsize=(10, 5))
    data = skewnorm.rvs(size=1000, a=5)
    sns.distplot(x[feature], ax=ax[0])
    sns.distplot(x[feature], ax=ax[1])
    ax[1].set_xscale("log")
    sns.distplot(np.log(data), ax=ax[2])
    ax[2].set_xticklabels([round(d, 1) for d in np.exp(ax[2].get_xticks())]);

for i in x.columns:
  Skew(i)



"""**Standardization**, **or** **mean** **removal** **and** **variance** **scaling**"""

scaler = preprocessing.StandardScaler().fit(x)
X_scaled = scaler.transform(x)

"""Data before Standarization"""

fig, ax = plt.subplots(1, 3, figsize=(10, 5))
data = skewnorm.rvs(size=1000, a=5)
sns.distplot(x, ax=ax[0])
sns.distplot(x, ax=ax[1])
ax[1].set_xscale("log")
sns.distplot(np.log(data), ax=ax[2])
ax[2].set_xticklabels([round(d, 1) for d in np.exp(ax[2].get_xticks())]);

"""Data after Standarization"""

fig, ax = plt.subplots(1, 3, figsize=(10, 5))
data = skewnorm.rvs(size=1000, a=5)
sns.distplot(X_scaled, ax=ax[0])
sns.distplot(X_scaled, ax=ax[1])
ax[1].set_xscale("log")
sns.distplot(np.log(data), ax=ax[2])
ax[2].set_xticklabels([round(d, 1) for d in np.exp(ax[2].get_xticks())]);

figure(figsize=(8,2), dpi=80)
#Creating plot
plt.boxplot(X_scaled)
# show plot
plt.show()

"""**Class** **Distribution**"""

sns.displot(y, multiple="dodge")

"""# **Preprocessing**

**Pearson** **correlation**
"""

corrMatrix= Data.corr()
fig, ax = plt.subplots(figsize=(10,10)) 
sns.heatmap(corrMatrix, annot=True, ax=ax)
plt.show()
print(corrMatrix)

#cp, thalach, exgang, oldpeak,

"""**Select** **K** **best**"""

# choose two features
# based on chi square statistical test 
# https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223

x_ci = SelectKBest(chi2, k=2).fit_transform(x, y) # we selected LPR, PEG

print(x_ci)
#The most two important features are : oldPeak and thalach

"""**Mutual** **Information** **Feature** **Selection**"""

from pandas.core.common import random_state
# feature selection
def select_features(x, y):
	# configure to select all features
	fs = SelectKBest(score_func=mutual_info_classif, k='all')
	# learn relationship from training data
	fs.fit(x, y)
	# transform train input data
	x_fs = fs.transform(x)
	return x_fs, fs
 
# feature selection
x_fs, fs = select_features(x, y)
# what are scores for the features
for i in range(len(fs.scores_)):
	print('Feature %d: %f' % (i, fs.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
pyplot.show()

#The best features are CP, exgang,oldpeak, ca, thal

"""**ANOVA** **f**-**test** **Feature** **Selection**"""

from pandas.core.common import random_state
# feature selection
def select_features(x, y):
	# configure to select all features
	fs = SelectKBest(score_func=f_classif, k='all')
	# learn relationship from training data
	fs.fit(x, y)
	# transform train input data
	x_fs = fs.transform(x)
	return x_fs, fs
 
# feature selection
x_fs, fs = select_features(x, y)
# what are scores for the features
for i in range(len(fs.scores_)):
	print('Feature %d: %f' % (i, fs.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
pyplot.show()

#The best features are CP, exgang,oldpeak, ca, thal

"""**Wrapper**: **Forward** **selection**"""

clf=RandomForestClassifier(max_depth=2, random_state=0)

"""# **Models**"""

x_train,x_test,y_train,y_test=train_test_split(X_scaled,y,test_size = 0.2, random_state =0)

print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)

def evaluation(y_test,y_pred, classifier):
  acc = accuracy_score(y_test,y_pred)
  print(acc)
  cm = confusion_matrix(y_test,y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)
  cr= classification_report(y_test,y_pred)
  print(cr)
  disp.plot()
  


def logestic_regression (x_train,x_test,y_train):
  lr = LogisticRegression(max_iter=1000, random_state=1, solver='liblinear', penalty='l1')
  lr.fit(x_train,y_train)
  y_pred_lr = lr.predict(x_test)
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_lr)
  #create ROC curve
  plt.plot(fpr,tpr)
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()
  return lr,y_pred_lr


def svm (x_train,x_test,y_train):
  classifier_svc = SVC(kernel="rbf")
  classifier_svc.fit(x_train,y_train)
  y_pred_svc = classifier_svc.predict(x_test)
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_svc)
  #create ROC curve
  plt.plot(fpr,tpr)
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()
  return classifier_svc, y_pred_svc


def RFC (x_train,x_test,y_train):
  classifier_RFC=RandomForestClassifier(max_depth=2, random_state=0)
  classifier_RFC.fit(x_train,y_train)
  y_pred_RFC=classifier_RFC.predict(x_test)
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_RFC)
  #create ROC curve
  plt.plot(fpr,tpr)
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()
  return classifier_RFC, y_pred_RFC

def DecisionTree(x_train,x_test,y_train):
  Tree=tree.DecisionTreeClassifier()
  Tree.fit(x_train,y_train)
  y_pred_DT=Tree.predict(x_test)
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_DT)
  #create ROC curve
  plt.plot(fpr,tpr)
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()
  return Tree,y_pred_DT

def XGBoost(x_train,x_test,y_train):
  XGB = xgb.XGBClassifier()
  XGB.fit(x_train, y_train)
  XGB_pred = XGB.predict(x_test)
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_DT)
  #create ROC curve
  plt.plot(fpr,tpr)
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()
  return XGB,XGB_pred

"""##SVM"""

classifier_svc, y_pred_svc=svm (x_train,x_test,y_train)
evaluation(y_test,y_pred_svc, classifier_svc)

"""##LR"""

lr, y_pred_lr=logestic_regression (x_train,x_test,y_train)
evaluation(y_test,y_pred_lr, lr)

"""##Random forest classifier"""

classifier_RFC, y_pred_RFC=RFC(x_train, x_test, y_train)
evaluation(y_test, y_pred_RFC, classifier_RFC)

"""##Decissiontree"""

Tree,y_pred_DT= DecisionTree(x_train,x_test,y_train)
evaluation(y_test,y_pred_DT,Tree)

"""##XGBOOST"""

XGB,XGB_pred_DT=XGBoost(x_train,x_test,y_train)
evaluation(y_test,XGB_pred_DT, XGB)

"""##voting"""

clf1 = LogisticRegression(random_state=0)

clf2 = RandomForestClassifier(n_estimators=10, 
                              max_depth=None,
                              min_samples_split=2, 
                              random_state=0)
clf3 = SVC(probability=True, kernel="rbf")
clf4 = DecisionTreeClassifier(max_depth=None,
                              min_samples_split=2, 
                              random_state=0)
clf5 = xgb.XGBClassifier()

"""**Soft** **voting**"""

eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3), ('DT', clf4), ('xgboost', clf5)], 
                        voting='soft'
                        )
eclf1 = eclf1.fit(x_train, np.ravel(y_train))
predictions = eclf1.predict(x_test)
cm = confusion_matrix(y_test, predictions, labels=eclf1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=eclf1.classes_)
disp.plot()
plt.show()
print(classification_report(predictions, y_test))

"""**Hard** **voting**"""

eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3), ('DT', clf4), ('xgboost', clf5)], 
                         voting='hard'
                         )
eclf2 = eclf2.fit(x_train, np.ravel(y_train))
predictions = eclf2.predict(x_test)
cm = confusion_matrix(y_test, predictions, labels=eclf1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                               display_labels=eclf1.classes_)
disp.plot()
plt.show()
print(classification_report(predictions, y_test))

"""#**Models + Feature selection**"""

x_train_df=pd.DataFrame(x_train)
x_test_df=pd.DataFrame(x_test)

"""##**SVM**: Forward selection"""

clf =SVC(kernel="rbf")
feature_selector = SequentialFeatureSelector(clf, n_features_to_select=5, direction='forward', scoring=None)
features = feature_selector.fit(np.array(x.fillna(0)), y)
feature_selector.get_support()
#cp(2), fbs(5), slope(10), ca(11), thal(12)

columns = [2, 5, 10, 11, 12]
train_svm = pd.DataFrame(x_train_df, columns=columns)
test_svm  = pd.DataFrame(x_test_df , columns=columns)

classifier_svc, y_pred_svc=svm (train_svm,test_svm,y_train)
evaluation(y_test,y_pred_svc, classifier_svc)

"""##**Logistic** **regression**: Forward selection"""

clf =LogisticRegression()
feature_selector = SequentialFeatureSelector(clf, n_features_to_select=5, direction='forward', scoring=None)
features = feature_selector.fit(np.array(x.fillna(0)), y)
feature_selector.get_support()
# sex(1), cp(2), exang(8), oldpeak(9), ca(11)

columns = [1, 2, 8, 9, 11]
train_lr = pd.DataFrame(x_train_df, columns=columns)
test_lr  = pd.DataFrame(x_test_df , columns=columns)

lr, y_pred_lr=logestic_regression (train_lr,test_lr,y_train)
evaluation(y_test,y_pred_lr, lr)

"""##**Rndom Forest**: Forward selection"""

clf =RandomForestClassifier(max_depth=2, random_state=0)
feature_selector = SequentialFeatureSelector(clf, n_features_to_select=5, direction='forward', scoring=None)
features = feature_selector.fit(np.array(x.fillna(0)), y)
feature_selector.get_support()
#sex(1), cp(2), slope(10), ca(11), thal(12)

columns = [1, 2, 10, 11, 12]
train_RFC = pd.DataFrame(x_train_df, columns=columns)
test_RFC  = pd.DataFrame(x_test_df , columns=columns)

classifier_RFC, y_pred_RFC=RFC(train_RFC, test_RFC, y_train)
evaluation(y_test, y_pred_RFC, classifier_RFC)

"""##**Decision** **tree**: Forward selection"""

clf=tree.DecisionTreeClassifier()
feature_selector = SequentialFeatureSelector(clf, n_features_to_select=5, direction='forward', scoring=None)
features = feature_selector.fit(np.array(x.fillna(0)), y)
feature_selector.get_support()
#sex(1), restecg(6), exang(8), ca(11), thal(12)

columns = [1, 6, 8, 11, 12]
train_DT = pd.DataFrame(x_train_df, columns=columns)
test_DT  = pd.DataFrame(x_test_df , columns=columns)

Tree,y_pred_DT= DecisionTree(train_DT,test_DT,y_train)
evaluation(y_test,y_pred_DT,Tree)

"""##**XGBoost**: Forward selection"""

clf=xgb.XGBClassifier()
feature_selector = SequentialFeatureSelector(clf, n_features_to_select=5, direction='forward', scoring=None)
features = feature_selector.fit(np.array(x.fillna(0)), y)
feature_selector.get_support()
#cp(2), fbs(5), slop(10), ca(11), thal(12)

columns = [2, 5, 10, 11, 12]
train_XGB = pd.DataFrame(x_train_df, columns=columns)
test_XGB  = pd.DataFrame(x_test_df, columns=columns)

XGB,XGB_pred_DT=XGBoost(train_XGB,test_XGB,y_train)
evaluation(y_test,XGB_pred_DT, XGB)

"""Voting

# Saving the model for deployment
"""

import pickle

#Prepare model file
classifier_svc=SVC(probability=True, kernel="rbf")
classifier_svc.fit(x_train, y_train)
data = {"model": classifier_svc, "scaler": scaler}
#save model file
with open('saved_steps.pkl', 'wb') as file:
    pickle.dump(data, file)

# Loadng the saved model to make sure it works
with open('saved_steps.pkl', 'rb') as file:
    data = pickle.load(file)

mdel_loaded = data["model"]
scaler_loaded = data["scaler"]

new_data = np.array([63,	1	,3,	145,	233,	1,	0,	150,	0,	2.3,	0,	0,	1]).reshape(1, -1)
print(new_data.shape)
new_X_scaled = scaler_loaded.transform(new_data)

mdel_loaded.predict_proba(new_X_scaled)

